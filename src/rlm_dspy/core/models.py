"""Model definitions for RLM-DSPy.

Auto-generated from models.dev API on 2026-02-05 15:41:39 UTC.
Do not edit manually - run: python -m rlm_dspy.scripts.generate_models

Provides model metadata including:
- Context window sizes
- Max output tokens  
- Pricing information
- Supported input types (text, image)
- Reasoning/thinking support
"""

from __future__ import annotations

from dataclasses import dataclass, field


@dataclass
class ModelCost:
    """Cost per million tokens in USD."""
    input: float = 0.0
    output: float = 0.0
    cache_read: float = 0.0
    cache_write: float = 0.0


@dataclass
class ModelInfo:
    """Model metadata."""
    id: str  # Full model ID (e.g., "anthropic/claude-sonnet-4-20250514")
    name: str  # Human-readable name
    provider: str  # Provider name (e.g., "anthropic", "openai")
    api: str  # API type (e.g., "anthropic", "openai", "openai-compatible")
    reasoning: bool = False  # Supports extended thinking
    input_types: list[str] = field(default_factory=lambda: ["text"])
    context_window: int = 128000
    max_tokens: int = 8192
    cost: ModelCost = field(default_factory=ModelCost)
    
    @property
    def supports_images(self) -> bool:
        return "image" in self.input_types


# ============================================================================
# Auto-generated Model Definitions
# ============================================================================

ANTHROPIC_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="anthropic/claude-3-5-haiku-20241022",
        name="Claude Haiku 3.5",
        provider="anthropic",
        api="anthropic",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=8192,
        cost=ModelCost(input=0.8, output=4, cache_read=0.08, cache_write=1),
    ),
    ModelInfo(
        id="anthropic/claude-3-5-haiku-latest",
        name="Claude Haiku 3.5 (latest)",
        provider="anthropic",
        api="anthropic",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=8192,
        cost=ModelCost(input=0.8, output=4, cache_read=0.08, cache_write=1),
    ),
    ModelInfo(
        id="anthropic/claude-3-5-sonnet-20240620",
        name="Claude Sonnet 3.5",
        provider="anthropic",
        api="anthropic",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=8192,
        cost=ModelCost(input=3, output=15, cache_read=0.3, cache_write=3.75),
    ),
    ModelInfo(
        id="anthropic/claude-3-5-sonnet-20241022",
        name="Claude Sonnet 3.5 v2",
        provider="anthropic",
        api="anthropic",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=8192,
        cost=ModelCost(input=3, output=15, cache_read=0.3, cache_write=3.75),
    ),
    ModelInfo(
        id="anthropic/claude-3-7-sonnet-20250219",
        name="Claude Sonnet 3.7",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=64000,
        cost=ModelCost(input=3, output=15, cache_read=0.3, cache_write=3.75),
    ),
    ModelInfo(
        id="anthropic/claude-3-7-sonnet-latest",
        name="Claude Sonnet 3.7 (latest)",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=64000,
        cost=ModelCost(input=3, output=15, cache_read=0.3, cache_write=3.75),
    ),
    ModelInfo(
        id="anthropic/claude-3-haiku-20240307",
        name="Claude Haiku 3",
        provider="anthropic",
        api="anthropic",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=4096,
        cost=ModelCost(input=0.25, output=1.25, cache_read=0.03, cache_write=0.3),
    ),
    ModelInfo(
        id="anthropic/claude-3-opus-20240229",
        name="Claude Opus 3",
        provider="anthropic",
        api="anthropic",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=4096,
        cost=ModelCost(input=15, output=75, cache_read=1.5, cache_write=18.75),
    ),
    ModelInfo(
        id="anthropic/claude-3-sonnet-20240229",
        name="Claude Sonnet 3",
        provider="anthropic",
        api="anthropic",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=4096,
        cost=ModelCost(input=3, output=15, cache_read=0.3, cache_write=0.3),
    ),
    ModelInfo(
        id="anthropic/claude-haiku-4-5",
        name="Claude Haiku 4.5 (latest)",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=64000,
        cost=ModelCost(input=1, output=5, cache_read=0.1, cache_write=1.25),
    ),
    ModelInfo(
        id="anthropic/claude-haiku-4-5-20251001",
        name="Claude Haiku 4.5",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=64000,
        cost=ModelCost(input=1, output=5, cache_read=0.1, cache_write=1.25),
    ),
    ModelInfo(
        id="anthropic/claude-opus-4-0",
        name="Claude Opus 4 (latest)",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=32000,
        cost=ModelCost(input=15, output=75, cache_read=1.5, cache_write=18.75),
    ),
    ModelInfo(
        id="anthropic/claude-opus-4-1",
        name="Claude Opus 4.1 (latest)",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=32000,
        cost=ModelCost(input=15, output=75, cache_read=1.5, cache_write=18.75),
    ),
    ModelInfo(
        id="anthropic/claude-opus-4-1-20250805",
        name="Claude Opus 4.1",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=32000,
        cost=ModelCost(input=15, output=75, cache_read=1.5, cache_write=18.75),
    ),
    ModelInfo(
        id="anthropic/claude-opus-4-20250514",
        name="Claude Opus 4",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=32000,
        cost=ModelCost(input=15, output=75, cache_read=1.5, cache_write=18.75),
    ),
    ModelInfo(
        id="anthropic/claude-opus-4-5",
        name="Claude Opus 4.5 (latest)",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=64000,
        cost=ModelCost(input=5, output=25, cache_read=0.5, cache_write=6.25),
    ),
    ModelInfo(
        id="anthropic/claude-opus-4-5-20251101",
        name="Claude Opus 4.5",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=64000,
        cost=ModelCost(input=5, output=25, cache_read=0.5, cache_write=6.25),
    ),
    ModelInfo(
        id="anthropic/claude-sonnet-4-0",
        name="Claude Sonnet 4 (latest)",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=64000,
        cost=ModelCost(input=3, output=15, cache_read=0.3, cache_write=3.75),
    ),
    ModelInfo(
        id="anthropic/claude-sonnet-4-20250514",
        name="Claude Sonnet 4",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=64000,
        cost=ModelCost(input=3, output=15, cache_read=0.3, cache_write=3.75),
    ),
    ModelInfo(
        id="anthropic/claude-sonnet-4-5",
        name="Claude Sonnet 4.5 (latest)",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=64000,
        cost=ModelCost(input=3, output=15, cache_read=0.3, cache_write=3.75),
    ),
    ModelInfo(
        id="anthropic/claude-sonnet-4-5-20250929",
        name="Claude Sonnet 4.5",
        provider="anthropic",
        api="anthropic",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=64000,
        cost=ModelCost(input=3, output=15, cache_read=0.3, cache_write=3.75),
    ),
]
OPENAI_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="openai/codex-mini-latest",
        name="Codex Mini",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text'],
        context_window=200000,
        max_tokens=100000,
        cost=ModelCost(input=1.5, output=6, cache_read=0.375, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-4",
        name="GPT-4",
        provider="openai",
        api="openai",
        reasoning=False,
        input_types=['text'],
        context_window=8192,
        max_tokens=8192,
        cost=ModelCost(input=30, output=60, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-4-turbo",
        name="GPT-4 Turbo",
        provider="openai",
        api="openai",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=4096,
        cost=ModelCost(input=10, output=30, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-4.1",
        name="GPT-4.1",
        provider="openai",
        api="openai",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=1047576,
        max_tokens=32768,
        cost=ModelCost(input=2, output=8, cache_read=0.5, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-4.1-mini",
        name="GPT-4.1 mini",
        provider="openai",
        api="openai",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=1047576,
        max_tokens=32768,
        cost=ModelCost(input=0.4, output=1.6, cache_read=0.1, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-4.1-nano",
        name="GPT-4.1 nano",
        provider="openai",
        api="openai",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=1047576,
        max_tokens=32768,
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.03, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-4o",
        name="GPT-4o",
        provider="openai",
        api="openai",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=16384,
        cost=ModelCost(input=2.5, output=10, cache_read=1.25, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-4o-2024-05-13",
        name="GPT-4o (2024-05-13)",
        provider="openai",
        api="openai",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=4096,
        cost=ModelCost(input=5, output=15, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-4o-2024-08-06",
        name="GPT-4o (2024-08-06)",
        provider="openai",
        api="openai",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=16384,
        cost=ModelCost(input=2.5, output=10, cache_read=1.25, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-4o-2024-11-20",
        name="GPT-4o (2024-11-20)",
        provider="openai",
        api="openai",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=16384,
        cost=ModelCost(input=2.5, output=10, cache_read=1.25, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-4o-mini",
        name="GPT-4o mini",
        provider="openai",
        api="openai",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=16384,
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.08, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5",
        name="GPT-5",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=128000,
        cost=ModelCost(input=1.25, output=10, cache_read=0.125, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5-codex",
        name="GPT-5-Codex",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=128000,
        cost=ModelCost(input=1.25, output=10, cache_read=0.125, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5-mini",
        name="GPT-5 Mini",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=128000,
        cost=ModelCost(input=0.25, output=2, cache_read=0.025, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5-nano",
        name="GPT-5 Nano",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=128000,
        cost=ModelCost(input=0.05, output=0.4, cache_read=0.005, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5-pro",
        name="GPT-5 Pro",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=272000,
        cost=ModelCost(input=15, output=120, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5.1",
        name="GPT-5.1",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=128000,
        cost=ModelCost(input=1.25, output=10, cache_read=0.13, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5.1-chat-latest",
        name="GPT-5.1 Chat",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=16384,
        cost=ModelCost(input=1.25, output=10, cache_read=0.125, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5.1-codex",
        name="GPT-5.1 Codex",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=128000,
        cost=ModelCost(input=1.25, output=10, cache_read=0.125, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5.1-codex-max",
        name="GPT-5.1 Codex Max",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=128000,
        cost=ModelCost(input=1.25, output=10, cache_read=0.125, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5.1-codex-mini",
        name="GPT-5.1 Codex mini",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=128000,
        cost=ModelCost(input=0.25, output=2, cache_read=0.025, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5.2",
        name="GPT-5.2",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=128000,
        cost=ModelCost(input=1.75, output=14, cache_read=0.175, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5.2-chat-latest",
        name="GPT-5.2 Chat",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=16384,
        cost=ModelCost(input=1.75, output=14, cache_read=0.175, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5.2-codex",
        name="GPT-5.2 Codex",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=128000,
        cost=ModelCost(input=1.75, output=14, cache_read=0.175, cache_write=0),
    ),
    ModelInfo(
        id="openai/gpt-5.2-pro",
        name="GPT-5.2 Pro",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=400000,
        max_tokens=128000,
        cost=ModelCost(input=21, output=168, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="openai/o1",
        name="o1",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=100000,
        cost=ModelCost(input=15, output=60, cache_read=7.5, cache_write=0),
    ),
    ModelInfo(
        id="openai/o1-pro",
        name="o1-pro",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=100000,
        cost=ModelCost(input=150, output=600, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="openai/o3",
        name="o3",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=100000,
        cost=ModelCost(input=2, output=8, cache_read=0.5, cache_write=0),
    ),
    ModelInfo(
        id="openai/o3-deep-research",
        name="o3-deep-research",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=100000,
        cost=ModelCost(input=10, output=40, cache_read=2.5, cache_write=0),
    ),
    ModelInfo(
        id="openai/o3-mini",
        name="o3-mini",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text'],
        context_window=200000,
        max_tokens=100000,
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.55, cache_write=0),
    ),
    ModelInfo(
        id="openai/o3-pro",
        name="o3-pro",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=100000,
        cost=ModelCost(input=20, output=80, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="openai/o4-mini",
        name="o4-mini",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=100000,
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.28, cache_write=0),
    ),
    ModelInfo(
        id="openai/o4-mini-deep-research",
        name="o4-mini-deep-research",
        provider="openai",
        api="openai",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=200000,
        max_tokens=100000,
        cost=ModelCost(input=2, output=8, cache_read=0.5, cache_write=0),
    ),
]
GOOGLE_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="google/gemini-1.5-flash",
        name="Gemini 1.5 Flash",
        provider="google",
        api="google",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=1000000,
        max_tokens=8192,
        cost=ModelCost(input=0.075, output=0.3, cache_read=0.01875, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-1.5-flash-8b",
        name="Gemini 1.5 Flash-8B",
        provider="google",
        api="google",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=1000000,
        max_tokens=8192,
        cost=ModelCost(input=0.0375, output=0.15, cache_read=0.01, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-1.5-pro",
        name="Gemini 1.5 Pro",
        provider="google",
        api="google",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=1000000,
        max_tokens=8192,
        cost=ModelCost(input=1.25, output=5, cache_read=0.3125, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.0-flash",
        name="Gemini 2.0 Flash",
        provider="google",
        api="google",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=8192,
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.025, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.0-flash-lite",
        name="Gemini 2.0 Flash Lite",
        provider="google",
        api="google",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=8192,
        cost=ModelCost(input=0.075, output=0.3, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.5-flash",
        name="Gemini 2.5 Flash",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.075, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.5-flash-lite",
        name="Gemini 2.5 Flash Lite",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.025, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.5-flash-lite-preview-06-17",
        name="Gemini 2.5 Flash Lite Preview 06-17",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.025, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.5-flash-lite-preview-09-2025",
        name="Gemini 2.5 Flash Lite Preview 09-25",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.025, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.5-flash-preview-04-17",
        name="Gemini 2.5 Flash Preview 04-17",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0375, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.5-flash-preview-05-20",
        name="Gemini 2.5 Flash Preview 05-20",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0375, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.5-flash-preview-09-2025",
        name="Gemini 2.5 Flash Preview 09-25",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.075, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.5-pro",
        name="Gemini 2.5 Pro",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=1.25, output=10, cache_read=0.31, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.5-pro-preview-05-06",
        name="Gemini 2.5 Pro Preview 05-06",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=1.25, output=10, cache_read=0.31, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-2.5-pro-preview-06-05",
        name="Gemini 2.5 Pro Preview 06-05",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=1.25, output=10, cache_read=0.31, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-3-flash-preview",
        name="Gemini 3 Flash Preview",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=0.5, output=3, cache_read=0.05, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-3-pro-preview",
        name="Gemini 3 Pro Preview",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1000000,
        max_tokens=64000,
        cost=ModelCost(input=2, output=12, cache_read=0.2, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-flash-latest",
        name="Gemini Flash Latest",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.075, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-flash-lite-latest",
        name="Gemini Flash-Lite Latest",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.025, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-live-2.5-flash",
        name="Gemini Live 2.5 Flash",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=8000,
        cost=ModelCost(input=0.5, output=2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="google/gemini-live-2.5-flash-preview-native-audio",
        name="Gemini Live 2.5 Flash Preview Native Audio",
        provider="google",
        api="google",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=65536,
        cost=ModelCost(input=0.5, output=2, cache_read=0, cache_write=0),
    ),
]
DEEPSEEK_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="deepseek/deepseek-chat",
        name="DeepSeek Chat",
        provider="deepseek",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=128000,
        max_tokens=8192,
        cost=ModelCost(input=0.28, output=0.42, cache_read=0.028, cache_write=0),
    ),
    ModelInfo(
        id="deepseek/deepseek-reasoner",
        name="DeepSeek Reasoner",
        provider="deepseek",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=128000,
        max_tokens=128000,
        cost=ModelCost(input=0.28, output=0.42, cache_read=0.028, cache_write=0),
    ),
]
GROQ_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="groq/deepseek-r1-distill-llama-70b",
        name="DeepSeek R1 Distill Llama 70B",
        provider="groq",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.75, output=0.99, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/gemma2-9b-it",
        name="Gemma 2 9B",
        provider="groq",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=8192,
        max_tokens=8192,
        cost=ModelCost(input=0.2, output=0.2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/llama-3.1-8b-instant",
        name="Llama 3.1 8B Instant",
        provider="groq",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=131072,
        cost=ModelCost(input=0.05, output=0.08, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/llama-3.3-70b-versatile",
        name="Llama 3.3 70B Versatile",
        provider="groq",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=32768,
        cost=ModelCost(input=0.59, output=0.79, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/llama3-70b-8192",
        name="Llama 3 70B",
        provider="groq",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=8192,
        max_tokens=8192,
        cost=ModelCost(input=0.59, output=0.79, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/llama3-8b-8192",
        name="Llama 3 8B",
        provider="groq",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=8192,
        max_tokens=8192,
        cost=ModelCost(input=0.05, output=0.08, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/meta-llama/llama-4-maverick-17b-128e-instruct",
        name="Llama 4 Maverick 17B",
        provider="groq",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.2, output=0.6, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/meta-llama/llama-4-scout-17b-16e-instruct",
        name="Llama 4 Scout 17B",
        provider="groq",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.11, output=0.34, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/mistral-saba-24b",
        name="Mistral Saba 24B",
        provider="groq",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=32768,
        max_tokens=32768,
        cost=ModelCost(input=0.79, output=0.79, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/moonshotai/kimi-k2-instruct",
        name="Kimi K2 Instruct",
        provider="groq",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=16384,
        cost=ModelCost(input=1, output=3, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/moonshotai/kimi-k2-instruct-0905",
        name="Kimi K2 Instruct 0905",
        provider="groq",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=262144,
        max_tokens=16384,
        cost=ModelCost(input=1, output=3, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/openai/gpt-oss-120b",
        name="GPT OSS 120B",
        provider="groq",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=65536,
        cost=ModelCost(input=0.15, output=0.6, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/openai/gpt-oss-20b",
        name="GPT OSS 20B",
        provider="groq",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=65536,
        cost=ModelCost(input=0.075, output=0.3, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/qwen-qwq-32b",
        name="Qwen QwQ 32B",
        provider="groq",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=16384,
        cost=ModelCost(input=0.29, output=0.39, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="groq/qwen/qwen3-32b",
        name="Qwen3 32B",
        provider="groq",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=16384,
        cost=ModelCost(input=0.29, output=0.59, cache_read=0, cache_write=0),
    ),
]
MISTRAL_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="mistral/codestral-latest",
        name="Codestral",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=256000,
        max_tokens=4096,
        cost=ModelCost(input=0.3, output=0.9, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/devstral-2512",
        name="Devstral 2",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=262144,
        max_tokens=262144,
        cost=ModelCost(input=0.4, output=2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/devstral-medium-2507",
        name="Devstral Medium",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=128000,
        max_tokens=128000,
        cost=ModelCost(input=0.4, output=2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/devstral-medium-latest",
        name="Devstral 2",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=262144,
        max_tokens=262144,
        cost=ModelCost(input=0.4, output=2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/devstral-small-2505",
        name="Devstral Small 2505",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=128000,
        max_tokens=128000,
        cost=ModelCost(input=0.1, output=0.3, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/devstral-small-2507",
        name="Devstral Small",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=128000,
        max_tokens=128000,
        cost=ModelCost(input=0.1, output=0.3, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/labs-devstral-small-2512",
        name="Devstral Small 2",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=256000,
        max_tokens=256000,
        cost=ModelCost(input=0, output=0, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/magistral-medium-latest",
        name="Magistral Medium",
        provider="mistral",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=128000,
        max_tokens=16384,
        cost=ModelCost(input=2, output=5, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/magistral-small",
        name="Magistral Small",
        provider="mistral",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=128000,
        max_tokens=128000,
        cost=ModelCost(input=0.5, output=1.5, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/ministral-3b-latest",
        name="Ministral 3B",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=128000,
        max_tokens=128000,
        cost=ModelCost(input=0.04, output=0.04, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/ministral-8b-latest",
        name="Ministral 8B",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=128000,
        max_tokens=128000,
        cost=ModelCost(input=0.1, output=0.1, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/mistral-large-2411",
        name="Mistral Large 2.1",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=16384,
        cost=ModelCost(input=2, output=6, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/mistral-large-2512",
        name="Mistral Large 3",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=262144,
        max_tokens=262144,
        cost=ModelCost(input=0.5, output=1.5, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/mistral-large-latest",
        name="Mistral Large",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=262144,
        max_tokens=262144,
        cost=ModelCost(input=0.5, output=1.5, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/mistral-medium-2505",
        name="Mistral Medium 3",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=131072,
        max_tokens=131072,
        cost=ModelCost(input=0.4, output=2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/mistral-medium-2508",
        name="Mistral Medium 3.1",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=262144,
        max_tokens=262144,
        cost=ModelCost(input=0.4, output=2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/mistral-medium-latest",
        name="Mistral Medium",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=16384,
        cost=ModelCost(input=0.4, output=2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/mistral-nemo",
        name="Mistral Nemo",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=128000,
        max_tokens=128000,
        cost=ModelCost(input=0.15, output=0.15, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/mistral-small-2506",
        name="Mistral Small 3.2",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=16384,
        cost=ModelCost(input=0.1, output=0.3, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/mistral-small-latest",
        name="Mistral Small",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=16384,
        cost=ModelCost(input=0.1, output=0.3, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/open-mistral-7b",
        name="Mistral 7B",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=8000,
        max_tokens=8000,
        cost=ModelCost(input=0.25, output=0.25, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/open-mixtral-8x22b",
        name="Mixtral 8x22B",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=64000,
        max_tokens=64000,
        cost=ModelCost(input=2, output=6, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/open-mixtral-8x7b",
        name="Mixtral 8x7B",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=32000,
        max_tokens=32000,
        cost=ModelCost(input=0.7, output=0.7, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/pixtral-12b",
        name="Pixtral 12B",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=128000,
        cost=ModelCost(input=0.15, output=0.15, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="mistral/pixtral-large-latest",
        name="Pixtral Large",
        provider="mistral",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=128000,
        cost=ModelCost(input=2, output=6, cache_read=0, cache_write=0),
    ),
]
COHERE_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="cohere/command-a-03-2025",
        name="Command A",
        provider="cohere",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=256000,
        max_tokens=8000,
        cost=ModelCost(input=2.5, output=10, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="cohere/command-a-reasoning-08-2025",
        name="Command A Reasoning",
        provider="cohere",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=256000,
        max_tokens=32000,
        cost=ModelCost(input=2.5, output=10, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="cohere/command-a-translate-08-2025",
        name="Command A Translate",
        provider="cohere",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=8000,
        max_tokens=8000,
        cost=ModelCost(input=2.5, output=10, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="cohere/command-r-08-2024",
        name="Command R",
        provider="cohere",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=128000,
        max_tokens=4000,
        cost=ModelCost(input=0.15, output=0.6, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="cohere/command-r-plus-08-2024",
        name="Command R+",
        provider="cohere",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=128000,
        max_tokens=4000,
        cost=ModelCost(input=2.5, output=10, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="cohere/command-r7b-12-2024",
        name="Command R7B",
        provider="cohere",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=128000,
        max_tokens=4000,
        cost=ModelCost(input=0.0375, output=0.15, cache_read=0, cache_write=0),
    ),
]
XAI_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="xai/grok-2",
        name="Grok 2",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=2, output=10, cache_read=2, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-2-1212",
        name="Grok 2 (1212)",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=2, output=10, cache_read=2, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-2-latest",
        name="Grok 2 Latest",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=2, output=10, cache_read=2, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-2-vision",
        name="Grok 2 Vision",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=8192,
        max_tokens=4096,
        cost=ModelCost(input=2, output=10, cache_read=2, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-2-vision-1212",
        name="Grok 2 Vision (1212)",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=8192,
        max_tokens=4096,
        cost=ModelCost(input=2, output=10, cache_read=2, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-2-vision-latest",
        name="Grok 2 Vision Latest",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=8192,
        max_tokens=4096,
        cost=ModelCost(input=2, output=10, cache_read=2, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-3",
        name="Grok 3",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=3, output=15, cache_read=0.75, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-3-fast",
        name="Grok 3 Fast",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=5, output=25, cache_read=1.25, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-3-fast-latest",
        name="Grok 3 Fast Latest",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=5, output=25, cache_read=1.25, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-3-latest",
        name="Grok 3 Latest",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=3, output=15, cache_read=0.75, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-3-mini",
        name="Grok 3 Mini",
        provider="xai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.3, output=0.5, cache_read=0.075, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-3-mini-fast",
        name="Grok 3 Mini Fast",
        provider="xai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.6, output=4, cache_read=0.15, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-3-mini-fast-latest",
        name="Grok 3 Mini Fast Latest",
        provider="xai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.6, output=4, cache_read=0.15, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-3-mini-latest",
        name="Grok 3 Mini Latest",
        provider="xai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.3, output=0.5, cache_read=0.075, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-4",
        name="Grok 4",
        provider="xai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=256000,
        max_tokens=64000,
        cost=ModelCost(input=3, output=15, cache_read=0.75, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-4-1-fast",
        name="Grok 4.1 Fast",
        provider="xai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=2000000,
        max_tokens=30000,
        cost=ModelCost(input=0.2, output=0.5, cache_read=0.05, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-4-1-fast-non-reasoning",
        name="Grok 4.1 Fast (Non-Reasoning)",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=2000000,
        max_tokens=30000,
        cost=ModelCost(input=0.2, output=0.5, cache_read=0.05, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-4-fast",
        name="Grok 4 Fast",
        provider="xai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=2000000,
        max_tokens=30000,
        cost=ModelCost(input=0.2, output=0.5, cache_read=0.05, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-4-fast-non-reasoning",
        name="Grok 4 Fast (Non-Reasoning)",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=2000000,
        max_tokens=30000,
        cost=ModelCost(input=0.2, output=0.5, cache_read=0.05, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-beta",
        name="Grok Beta",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=4096,
        cost=ModelCost(input=5, output=15, cache_read=5, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-code-fast-1",
        name="Grok Code Fast 1",
        provider="xai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=256000,
        max_tokens=10000,
        cost=ModelCost(input=0.2, output=1.5, cache_read=0.02, cache_write=0),
    ),
    ModelInfo(
        id="xai/grok-vision-beta",
        name="Grok Vision Beta",
        provider="xai",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=8192,
        max_tokens=4096,
        cost=ModelCost(input=5, output=15, cache_read=5, cache_write=0),
    ),
]
MINIMAX_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="minimax/MiniMax-M2",
        name="MiniMax-M2",
        provider="minimax",
        api="anthropic-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=196608,
        max_tokens=128000,
        cost=ModelCost(input=0.3, output=1.2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="minimax/MiniMax-M2.1",
        name="MiniMax-M2.1",
        provider="minimax",
        api="anthropic-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=204800,
        max_tokens=131072,
        cost=ModelCost(input=0.3, output=1.2, cache_read=0, cache_write=0),
    ),
]
MINIMAX_CN_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="minimax-cn/MiniMax-M2",
        name="MiniMax-M2",
        provider="minimax-cn",
        api="anthropic-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=196608,
        max_tokens=128000,
        cost=ModelCost(input=0.3, output=1.2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="minimax-cn/MiniMax-M2.1",
        name="MiniMax-M2.1",
        provider="minimax-cn",
        api="anthropic-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=204800,
        max_tokens=131072,
        cost=ModelCost(input=0.3, output=1.2, cache_read=0, cache_write=0),
    ),
]
KIMI_FOR_CODING_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="kimi-for-coding/k2p5",
        name="Kimi K2.5",
        provider="kimi-for-coding",
        api="anthropic-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=262144,
        max_tokens=32768,
        cost=ModelCost(input=0, output=0, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="kimi-for-coding/kimi-k2-thinking",
        name="Kimi K2 Thinking",
        provider="kimi-for-coding",
        api="anthropic-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=262144,
        max_tokens=32768,
        cost=ModelCost(input=0, output=0, cache_read=0, cache_write=0),
    ),
]
ZAI_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="zai/glm-4.5",
        name="GLM-4.5",
        provider="zai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=98304,
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.11, cache_write=0),
    ),
    ModelInfo(
        id="zai/glm-4.5-air",
        name="GLM-4.5-Air",
        provider="zai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=98304,
        cost=ModelCost(input=0.2, output=1.1, cache_read=0.03, cache_write=0),
    ),
    ModelInfo(
        id="zai/glm-4.5-flash",
        name="GLM-4.5-Flash",
        provider="zai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=98304,
        cost=ModelCost(input=0, output=0, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="zai/glm-4.5v",
        name="GLM-4.5V",
        provider="zai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=64000,
        max_tokens=16384,
        cost=ModelCost(input=0.6, output=1.8, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="zai/glm-4.6",
        name="GLM-4.6",
        provider="zai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=204800,
        max_tokens=131072,
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.11, cache_write=0),
    ),
    ModelInfo(
        id="zai/glm-4.6v",
        name="GLM-4.6V",
        provider="zai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=32768,
        cost=ModelCost(input=0.3, output=0.9, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="zai/glm-4.7",
        name="GLM-4.7",
        provider="zai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=204800,
        max_tokens=131072,
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.11, cache_write=0),
    ),
    ModelInfo(
        id="zai/glm-4.7-flash",
        name="GLM-4.7-Flash",
        provider="zai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=200000,
        max_tokens=131072,
        cost=ModelCost(input=0, output=0, cache_read=0, cache_write=0),
    ),
]
ZHIPUAI_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="zhipuai/glm-4.5",
        name="GLM-4.5",
        provider="zhipuai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=98304,
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.11, cache_write=0),
    ),
    ModelInfo(
        id="zhipuai/glm-4.5-air",
        name="GLM-4.5-Air",
        provider="zhipuai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=98304,
        cost=ModelCost(input=0.2, output=1.1, cache_read=0.03, cache_write=0),
    ),
    ModelInfo(
        id="zhipuai/glm-4.5-flash",
        name="GLM-4.5-Flash",
        provider="zhipuai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=98304,
        cost=ModelCost(input=0, output=0, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="zhipuai/glm-4.5v",
        name="GLM-4.5V",
        provider="zhipuai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=64000,
        max_tokens=16384,
        cost=ModelCost(input=0.6, output=1.8, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="zhipuai/glm-4.6",
        name="GLM-4.6",
        provider="zhipuai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=204800,
        max_tokens=131072,
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.11, cache_write=0),
    ),
    ModelInfo(
        id="zhipuai/glm-4.6v",
        name="GLM-4.6V",
        provider="zhipuai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=128000,
        max_tokens=32768,
        cost=ModelCost(input=0.3, output=0.9, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="zhipuai/glm-4.7",
        name="GLM-4.7",
        provider="zhipuai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=204800,
        max_tokens=131072,
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.11, cache_write=0),
    ),
    ModelInfo(
        id="zhipuai/glm-4.7-flash",
        name="GLM-4.7-Flash",
        provider="zhipuai",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=200000,
        max_tokens=131072,
        cost=ModelCost(input=0, output=0, cache_read=0, cache_write=0),
    ),
]
ALIBABA_MODELS: list[ModelInfo] = [
    ModelInfo(
        id="alibaba/qvq-max",
        name="QVQ Max",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=1.2, output=4.8, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen-flash",
        name="Qwen Flash",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=1000000,
        max_tokens=32768,
        cost=ModelCost(input=0.05, output=0.4, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen-max",
        name="Qwen Max",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=32768,
        max_tokens=8192,
        cost=ModelCost(input=1.6, output=6.4, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen-omni-turbo",
        name="Qwen-Omni Turbo",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=32768,
        max_tokens=2048,
        cost=ModelCost(input=0.07, output=0.27, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen-omni-turbo-realtime",
        name="Qwen-Omni Turbo Realtime",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=32768,
        max_tokens=2048,
        cost=ModelCost(input=0.27, output=1.07, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen-plus",
        name="Qwen Plus",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=1000000,
        max_tokens=32768,
        cost=ModelCost(input=0.4, output=1.2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen-plus-character-ja",
        name="Qwen Plus Character (Japanese)",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=8192,
        max_tokens=512,
        cost=ModelCost(input=0.5, output=1.4, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen-turbo",
        name="Qwen Turbo",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=1000000,
        max_tokens=16384,
        cost=ModelCost(input=0.05, output=0.2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen-vl-max",
        name="Qwen-VL Max",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.8, output=3.2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen-vl-plus",
        name="Qwen-VL Plus",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.21, output=0.63, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen2-5-14b-instruct",
        name="Qwen2.5 14B Instruct",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.35, output=1.4, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen2-5-32b-instruct",
        name="Qwen2.5 32B Instruct",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.7, output=2.8, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen2-5-72b-instruct",
        name="Qwen2.5 72B Instruct",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=1.4, output=5.6, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen2-5-7b-instruct",
        name="Qwen2.5 7B Instruct",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.175, output=0.7, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen2-5-omni-7b",
        name="Qwen2.5-Omni 7B",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=32768,
        max_tokens=2048,
        cost=ModelCost(input=0.1, output=0.4, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen2-5-vl-72b-instruct",
        name="Qwen2.5-VL 72B Instruct",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=2.8, output=8.4, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen2-5-vl-7b-instruct",
        name="Qwen2.5-VL 7B Instruct",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.35, output=1.05, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-14b",
        name="Qwen3 14B",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.35, output=1.4, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-235b-a22b",
        name="Qwen3 235B-A22B",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=16384,
        cost=ModelCost(input=0.7, output=2.8, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-32b",
        name="Qwen3 32B",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=16384,
        cost=ModelCost(input=0.7, output=2.8, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-8b",
        name="Qwen3 8B",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.18, output=0.7, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-coder-30b-a3b-instruct",
        name="Qwen3-Coder 30B-A3B Instruct",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=262144,
        max_tokens=65536,
        cost=ModelCost(input=0.45, output=2.25, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-coder-480b-a35b-instruct",
        name="Qwen3-Coder 480B-A35B Instruct",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=262144,
        max_tokens=65536,
        cost=ModelCost(input=1.5, output=7.5, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-coder-flash",
        name="Qwen3 Coder Flash",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=1000000,
        max_tokens=65536,
        cost=ModelCost(input=0.3, output=1.5, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-coder-plus",
        name="Qwen3 Coder Plus",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=1048576,
        max_tokens=65536,
        cost=ModelCost(input=1, output=5, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-max",
        name="Qwen3 Max",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=262144,
        max_tokens=65536,
        cost=ModelCost(input=1.2, output=6, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-next-80b-a3b-instruct",
        name="Qwen3-Next 80B-A3B Instruct",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text'],
        context_window=131072,
        max_tokens=32768,
        cost=ModelCost(input=0.5, output=2, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-next-80b-a3b-thinking",
        name="Qwen3-Next 80B-A3B (Thinking)",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=32768,
        cost=ModelCost(input=0.5, output=6, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-omni-flash",
        name="Qwen3-Omni Flash",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=65536,
        max_tokens=16384,
        cost=ModelCost(input=0.43, output=1.66, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-omni-flash-realtime",
        name="Qwen3-Omni Flash Realtime",
        provider="alibaba",
        api="openai-compatible",
        reasoning=False,
        input_types=['text', 'image'],
        context_window=65536,
        max_tokens=16384,
        cost=ModelCost(input=0.52, output=1.99, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-vl-235b-a22b",
        name="Qwen3-VL 235B-A22B",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=131072,
        max_tokens=32768,
        cost=ModelCost(input=0.7, output=2.8, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-vl-30b-a3b",
        name="Qwen3-VL 30B-A3B",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=131072,
        max_tokens=32768,
        cost=ModelCost(input=0.2, output=0.8, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwen3-vl-plus",
        name="Qwen3-VL Plus",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text', 'image'],
        context_window=262144,
        max_tokens=32768,
        cost=ModelCost(input=0.2, output=1.6, cache_read=0, cache_write=0),
    ),
    ModelInfo(
        id="alibaba/qwq-plus",
        name="QwQ Plus",
        provider="alibaba",
        api="openai-compatible",
        reasoning=True,
        input_types=['text'],
        context_window=131072,
        max_tokens=8192,
        cost=ModelCost(input=0.8, output=2.4, cache_read=0, cache_write=0),
    ),
]


# ============================================================================
# Model Registry
# ============================================================================

ALL_MODELS: list[ModelInfo] = [
    *ANTHROPIC_MODELS,
    *OPENAI_MODELS,
    *GOOGLE_MODELS,
    *DEEPSEEK_MODELS,
    *GROQ_MODELS,
    *MISTRAL_MODELS,
    *COHERE_MODELS,
    *XAI_MODELS,
    *MINIMAX_MODELS,
    *MINIMAX_CN_MODELS,
    *KIMI_FOR_CODING_MODELS,
    *ZAI_MODELS,
    *ZHIPUAI_MODELS,
    *ALIBABA_MODELS,
]

_MODEL_BY_ID: dict[str, ModelInfo] = {m.id: m for m in ALL_MODELS}


def find_model(model_id: str) -> ModelInfo | None:
    """Find model info by ID.
    
    Args:
        model_id: Full model ID (e.g., "anthropic/claude-sonnet-4-20250514")
        
    Returns:
        ModelInfo if found, None otherwise
    """
    return _MODEL_BY_ID.get(model_id)


def list_models(provider: str | None = None) -> list[ModelInfo]:
    """List all available models, optionally filtered by provider.
    
    Args:
        provider: Optional provider filter (e.g., "anthropic", "openai")
        
    Returns:
        List of ModelInfo objects
    """
    if provider:
        return [m for m in ALL_MODELS if m.provider == provider]
    return ALL_MODELS.copy()


def get_model_ids(provider: str | None = None) -> list[str]:
    """Get list of model IDs, optionally filtered by provider."""
    return [m.id for m in list_models(provider)]

# ============================================================================
# RLM-DSPy Environment Variables
# ============================================================================
# Copy this file to .env and customize for your setup.
# All variables are optional - sensible defaults are provided.

# ============================================================================
# API Configuration
# ============================================================================

# API key for LLM provider (required for API calls)
# Supports: OpenRouter, OpenAI, Anthropic, etc.
RLM_API_KEY=sk-or-v1-your-api-key-here

# Alternative: OpenRouter-specific key (fallback if RLM_API_KEY not set)
# OPENROUTER_API_KEY=sk-or-v1-your-openrouter-key

# API endpoint (default: OpenRouter)
# Examples:
#   OpenRouter: https://openrouter.ai/api/v1
#   OpenAI:     https://api.openai.com/v1
#   Anthropic:  https://api.anthropic.com/v1
#   Local:      http://localhost:8000/v1
RLM_API_BASE=https://openrouter.ai/api/v1

# ============================================================================
# Model Selection
# ============================================================================

# Primary model for main analysis (auto-prefixed with openrouter/ if needed)
# Popular choices:
#   google/gemini-2.0-flash-exp     - Fast, cheap, 1M context
#   google/gemini-1.5-pro           - Best quality, 2M context
#   anthropic/claude-sonnet-4       - High quality reasoning
#   anthropic/claude-3-haiku        - Fast and cheap
#   openai/gpt-4o                   - GPT-4 Omni
#   openai/gpt-4o-mini              - Fast GPT-4
RLM_MODEL=google/gemini-2.0-flash-exp

# Sub-model for lightweight tasks (chunk analysis, sub-queries)
# Use a faster/cheaper model here for cost savings
# Default: same as RLM_MODEL
RLM_SUB_MODEL=google/gemini-2.0-flash-exp

# ============================================================================
# Execution Limits
# ============================================================================

# Maximum cost in USD before stopping (default: 1.0)
RLM_MAX_BUDGET=1.0

# Maximum execution time in seconds (default: 300)
RLM_MAX_TIMEOUT=300

# Maximum tokens to process (default: 500000)
RLM_MAX_TOKENS=500000

# Maximum recursive iterations (default: 30)
RLM_MAX_ITERATIONS=30

# Maximum recursion depth for hierarchical processing (default: 3)
RLM_MAX_DEPTH=3

# ============================================================================
# Chunking Configuration
# ============================================================================

# Default chunk size in characters (default: 100000)
# Larger = fewer API calls but may hit context limits
# Smaller = more parallel processing but higher overhead
RLM_CHUNK_SIZE=100000

# Overlap between chunks in characters (default: 500)
# Helps maintain context across chunk boundaries
RLM_OVERLAP=500

# Enable syntax-aware chunking using tree-sitter (default: true)
# When enabled, chunks at function/class boundaries instead of arbitrary positions
# Prevents false positives from truncated code in LLM analysis
# Requires: pip install tree-sitter tree-sitter-python (etc.)
RLM_SYNTAX_AWARE=true

# Threshold for paste store in characters (default: 2000)
# Content exceeding this is stored with placeholder to prevent context overflow
RLM_PASTE_THRESHOLD=2000

# ============================================================================
# Processing Configuration
# ============================================================================

# Maximum concurrent chunk processing (default: 20)
# Higher = faster but more API rate limit pressure
RLM_PARALLEL_CHUNKS=20

# Use async HTTP client for parallel requests (default: true)
RLM_USE_ASYNC=true

# Use DSPy compiled/optimized prompts (default: true)
RLM_USE_COMPILED_PROMPTS=true

# ============================================================================
# Model-Specific Settings
# ============================================================================

# Disable extended thinking for supported models (default: true)
# Set to false if you want models like Claude to use extended thinking
RLM_DISABLE_THINKING=true

# Enable prompt caching for supported providers (default: true)
# Reduces costs for repeated similar queries
RLM_ENABLE_CACHE=true

# ============================================================================
# Logging & Debug
# ============================================================================

# Verbosity levels (only one should be true):

# Quiet mode - minimal output
# RLM_QUIET=true

# Normal mode (default) - standard output
# (no variable needed, this is the default)

# Verbose mode - detailed progress info
# RLM_VERBOSE=true

# Debug mode - full debug output including API requests/responses
# RLM_DEBUG=true

# ============================================================================
# Storage
# ============================================================================

# Directory for batch processing files (default: system temp dir)
# RLM_BATCH_DIR=/tmp/rlm_batches

# ============================================================================
# Example Configurations
# ============================================================================

# --- Cost-Optimized (cheapest) ---
# RLM_MODEL=google/gemini-2.0-flash-exp
# RLM_SUB_MODEL=google/gemini-2.0-flash-exp
# RLM_MAX_BUDGET=0.10
# RLM_CHUNK_SIZE=150000

# --- Quality-Optimized (best results) ---
# RLM_MODEL=anthropic/claude-sonnet-4
# RLM_SUB_MODEL=anthropic/claude-3-haiku
# RLM_MAX_BUDGET=5.0
# RLM_CHUNK_SIZE=80000

# --- Speed-Optimized (fastest) ---
# RLM_MODEL=google/gemini-2.0-flash-exp
# RLM_PARALLEL_CHUNKS=50
# RLM_CHUNK_SIZE=50000
# RLM_USE_ASYNC=true

# --- Large Codebase (2M+ tokens) ---
# RLM_MODEL=google/gemini-1.5-pro
# RLM_MAX_TOKENS=2000000
# RLM_CHUNK_SIZE=200000
# RLM_MAX_DEPTH=5
